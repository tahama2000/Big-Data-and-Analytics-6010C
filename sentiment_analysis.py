# -*- coding: utf-8 -*-
"""Sentiment_Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IBv1ts45KOQxD8B1dLvMqSdHWCHFdu4L
"""

!pip install -U -q PyDrive
import os
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

local_download_path = os.path.expanduser('~/data')
try:
  os.makedirs(local_download_path)
except: pass

# 2. Auto-iterate using the query syntax
#    https://developers.google.com/drive/v2/web/search-parameters
file_list = drive.ListFile(
    {'q': "'1EDEvz9CFck1YTjQc5LqHzWkWL7PZgZ3R' in parents"}).GetList()

for f in file_list:
  # 3. Create & download by id.
  print('title: %s, id: %s' % (f['title'], f['id']))
  fname = os.path.join(local_download_path, f['title'])
  print('downloading to {}'.format(fname))
  f_ = drive.CreateFile({'id': f['id']})
  f_.GetContentFile(fname)

pip install pycld2

!pip install langdetect

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd

#NLP Library
import spacy
import re

#Language Detect Library
import pycld2 as cld2

#Feature extraction, model selection and model training library
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split

#Libraries to check the model performance
from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

#Graphing Libraries
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as snb
from wordcloud import WordCloud, STOPWORDS

#Hypothesis Testing
import math
from scipy.stats import chi2_contingency, chisquare, chi2

#Sentiment Analysis Libraries
from nltk.sentiment import SentimentAnalyzer
from nltk.sentiment.util import *
from nltk.sentiment.vader import SentimentIntensityAnalyzer

nltk.download('vader_lexicon')

nlp = spacy.load("en_core_web_sm",disable=["tagger","parser","ner"])

def clean_tweet_text_regex(tweet):
    tweet = tweet.lower() # to lower case
    tweet = re.sub(r"@[A-Za-z0-9]+","", tweet) # Remove @Mentions
    tweet = re.sub(r"#","", tweet) # remove # \ART[\s]+
    tweet = re.sub(r"\ART[\s]+","", tweet) # Remove RT in start
    tweet = re.sub(r"https?:\/\/\S+","", tweet) # Remove hyperlink
    tweet = re.sub("(.)\\1{2,}","\\1", tweet) # Remove more than one characters.
    tweet = re.sub(r"[^A-Za-z0-9\s]+", "",str(tweet)) #remove special characters "!"
    
    return tweet

def clean_tweet_spacy_core(doc):
    tokens= []

    for token in doc:
        if token.is_stop:
            continue
        if token.text != token.lemma_:
            tokens.append(token.lemma_)
        else:
            tokens.append(token.text)
      
    
    tweet=" ".join(tokens)
    return tweet

def doc_to_spans(list_of_texts, join_string=' ||| '):
    all_docs = nlp(' ||| '.join(list_of_texts))
    split_inds = [i for i, token in enumerate(all_docs) if token.text == '|||'] + [len(all_docs)]
    new_docs = [all_docs[(i + 1 if i > 0 else i):j] for i, j in zip([0] + split_inds[:-1], split_inds)]
    all_docs = None
    return new_docs 

def get_complete_spans(data,colname):
    splitted_frames = np.array_split(data, 500)
    docs=[]
    i = 1
    for frame in splitted_frames:
        docs.append(doc_to_spans(frame[colname]))
    return docs

def clean_tweet_spacy(data, colname):
    docs = get_complete_spans(data, colname)
    
    print("Tweets cleaning started")
    tweets = []
    
    for doces in docs:
        for doc in doces:
            tweets.append(clean_tweet_spacy_core(doc))

    print("Tweets cleanned")
    return tweets

def detect_lang(tweet):
    try:
        isReliable, textBytesFound, details = cld2.detect(tweet)
        return details[0][0]
    except:
        return "not found"

def check_col_null_values(data):
    col_null_values = pd.DataFrame(columns=["column","nullvalues"])
    null_values = []

    for col in data.columns:
        sum = data[col].isnull().sum()
        print(f"{col}: null percent: {sum/data.shape[0]*100} null values: {sum}/{data.shape[0]} value count: {data.shape[0] - sum}/{data.shape[0]}")
        null_values.append(sum)
    
    col_null_values["column"] = data.columns
    col_null_values["nullvalues"] = null_values

    return col_null_values

from google.colab import drive
drive.mount('/content/drive')

tweets = pd.read_csv("/content/drive/MyDrive/election2020tweetsConsolidated.csv", encoding = "ISO-8859-1", engine="python")

tweets.head()

tweets.columns

col_null_values = check_col_null_values(tweets)

tweets.dtypes

def clean_data_frame(dataframe):
    dataframe['likes'] = pd.to_numeric(dataframe['likes'], errors="coerce")
    dataframe['user_followers_count'] = pd.to_numeric(dataframe['user_followers_count'], errors="coerce")
    dataframe["likes"].fillna(dataframe["likes"].mean(), inplace=True)
    dataframe["retweet_count"].fillna(dataframe["retweet_count"].mean(), inplace=True)
    dataframe["user_followers_count"].fillna(dataframe["user_followers_count"].mean(), inplace=True)
    dataframe['created_at'] = pd.to_datetime(dataframe['created_at'], errors="coerce") 
    dataframe["created_at"] = dataframe["created_at"].dt.strftime('%m-%d')

clean_data_frame(tweets)

np.sum(tweets["tweet"].isnull())

tweets = tweets.dropna(subset=['tweet'])

check_col_null_values(tweets)

tweets["tweet"] = tweets["tweet"].apply(clean_tweet_text_regex)

tweet_langs = tweets["tweet"].apply(detect_lang)

tweet_langs.unique()

tweets["Lang"] = tweet_langs

eng_tweets = tweets[tweets["Lang"] == "ENGLISH"]

eng_tweets["tweet"] = clean_tweet_spacy(eng_tweets, "tweet")

print(eng_tweets)

def sentiment(data):
    temp=[]
    for row in data:
        tmp=sid.polarity_scores(row)
        temp.append(tmp)
    return temp

sid = SentimentIntensityAnalyzer()
eng_tweets['VADAR'] = sentiment(eng_tweets['tweet'])
eng_tweets['compound']  = eng_tweets['VADAR'].apply(lambda score_dict: score_dict['compound'])
eng_tweets['sentiment']  = eng_tweets['compound'].apply(lambda x: 'pos' if x > 0.05 else ('neg' if x < -0.05 else 'neu'))

print(eng_tweets)

eng_tweets.to_csv(r'F:\labeled_tweets.csv')